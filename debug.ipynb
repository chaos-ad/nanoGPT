{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm\n",
    "# %pip install ipywidgets\n",
    "# %pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.auto as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab: {''.join(chars)}\")\n",
    "print(f\"{len(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our own encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx = {}\n",
    "idx2token = {}\n",
    "\n",
    "for idx, token in enumerate(sorted(list(set(text)))):\n",
    "    token2idx[token] = idx\n",
    "    idx2token[idx] = token\n",
    "\n",
    "def encode_one(token):\n",
    "    return token2idx[token]\n",
    "\n",
    "def decode_one(idx):\n",
    "    return idx2token[idx]\n",
    "\n",
    "def encode(text):\n",
    "    return [encode_one(char) for char in text]\n",
    "\n",
    "def decode(code):\n",
    "    return \"\".join([decode_one(idx) for idx in code])\n",
    "\n",
    "vocab_size = len(token2idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google/sentencepiece - Subword Unit Level Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TikToken - Byte Pair Token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.encode(\"hii there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc.decode([71, 4178, 612])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "text_tensor = torch.tensor(encode(text))\n",
    "text_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "print(text_tensor.shape, text_tensor.dtype)\n",
    "print(text_tensor[:10]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(text_tensor))\n",
    "train_data = text_tensor[:n]\n",
    "val_data = text_tensor[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never give all data, only work on chunks of data, sampled from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "# aka context_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Cit'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:block_size+1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna train our transformer to simultaneously predict all next characters, so previous block gives us 8 different examples.\n",
    "We want our transformer to see different contexts up to and including block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117a29f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split == \"train\" else val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 0, 0, 3, 0, 0, 4, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(high=5, size=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs.shape=torch.Size([4, 8])\n",
      "ys.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split=\"train\", batch_size=4, block_size=8, device=None):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    batch_offsets = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "    xs = torch.stack([data[idx:idx+block_size] for idx in batch_offsets])\n",
    "    ys = torch.stack([data[idx+1:idx+block_size+1] for idx in batch_offsets])\n",
    "    if device:\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = get_batch()\n",
    "print(f\"{xs.shape=}\")\n",
    "print(f\"{ys.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 58, 47, 53, 52, 12,  1, 37],\n",
       "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
       "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
       "        [56, 53, 63,  1, 42, 47, 42,  1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58, 47, 53, 52, 12,  1, 37, 53],\n",
       "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
       "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
       "        [53, 63,  1, 42, 47, 42,  1, 57]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117a29f10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65, 65)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(vocab_size, vocab_size)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1338,  0.3899, -0.2884, -1.4651,  0.0101, -0.3004, -1.5733,  0.0148,\n",
       "         -0.0447, -0.5367, -0.5223, -0.2181, -2.1608,  0.7865,  0.6854, -1.2576,\n",
       "          0.6094, -2.0551, -0.4431, -0.6499, -0.6870,  0.2567, -1.2669,  0.2645,\n",
       "         -0.6445,  1.0834, -0.7995,  0.2922,  1.3143,  1.2607, -0.3505, -2.0660,\n",
       "          1.0575, -1.0572,  0.9911, -0.0797,  1.0751,  0.2381,  0.5757,  1.6685,\n",
       "          0.5976, -1.8736,  1.2910, -0.3753, -1.8943,  0.5557,  0.8567, -0.8461,\n",
       "          0.5015, -0.9656, -0.7255,  0.0990,  0.5928, -0.0422, -0.9566,  1.4424,\n",
       "          0.4341, -0.4292,  0.3666,  0.1275, -0.0560,  0.8315, -0.5512,  1.0477,\n",
       "          1.6187]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        logits = self.token_emb(inputs) ## logits.shape=(B,T,C)\n",
    "        # logits = torch.reshape(logits, (-1, logits.shape[-1]))\n",
    "        # targets = torch.reshape(targets, (-1,))\n",
    "        (B,T,C) = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([4, 8, 65])\n",
      "loss=tensor(4.6267, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "out, loss = model(xs, ys)\n",
    "print(f\"{out.shape=}\")\n",
    "print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a generation from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape=torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs = xs\n",
    "print(f\"{inputs.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(xs, ys)\n",
    "print(f\"{logits.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape=torch.Size([4, 65])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "print(f\"{probs.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.shape=torch.Size([4, 1])\n",
      "sample=tensor([[12],\n",
      "        [42],\n",
      "        [60],\n",
      "        [55]])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.multinomial(probs, 1)\n",
    "print(f\"{sample.shape=}\")\n",
    "print(f\"{sample=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 58, 47, 53, 52, 12,  1, 37, 12],\n",
       "        [53, 56, 43,  1, 21,  1, 41, 39, 42],\n",
       "        [50, 39, 52, 63,  1, 47, 58, 57, 60],\n",
       "        [56, 53, 63,  1, 42, 47, 42,  1, 55]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.concat([inputs, sample], dim=-1)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        logits = self.token_emb(inputs) ## logits.shape=(B,T,C)\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, inputs, sequence_length = 100, context_size = None):\n",
    "    \"\"\"\n",
    "    inputs: input sequence that is used to start generation\n",
    "    size: length of a generated sequence\n",
    "    \"\"\"\n",
    "    result = inputs\n",
    "    for _ in range(sequence_length):\n",
    "        ## Get the predictions over the next character\n",
    "        if context_size is None:\n",
    "            cond = result\n",
    "        else:\n",
    "            cond = result[:, -context_size:] # Trim the context\n",
    "        logits = model(cond)\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "        sample = torch.multinomial(probs, num_samples=1)\n",
    "        result = torch.concat([result, sample], dim=-1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([4, 8, 65])\n",
      "loss=tensor(4.2725, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "logits, loss = model(xs, ys)\n",
    "print(f\"{logits.shape=}\")\n",
    "print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 58, 47, 53, 52, 12,  1, 37],\n",
       "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
       "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
       "        [56, 53, 63,  1, 42, 47, 42,  1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 58, 47, 53, 52, 12,  1, 37,  2, 18, 36, 11, 21])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, xs, 5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!bPk ;M-.\n",
      ",ruyhZoO:SVV:VVOt:e$Ie,&3Wr!dhTx;ldKBNL:d,MbBAOe JYR\n",
      "&:rNHUESAbIfa!S\n",
      "h;q:kUoKYy\n",
      "QlmsKuMul3U\n",
      "sjOdXEBFckMbBu&Ud$oP?fNDMCkgcRPgY,Zk\n",
      "BSCsAOE:rnhDw.Sam$,&EFARCXzKGWkb'K'3Xl.:fwBFQpKB;-tHMRf!LcXHRmR$,Qxkm:ffwlxud\n",
      "dDmRDnM;HNB'K?XKSUfcpE-SChhuKF\n",
      "EtO;P?Wy,Qb'KMWcZmomfqubhumeqaVHuxbJQlKyDTA!oS\n",
      "EGDMmOF'l;hYgK;ibFMRf!y'NlDMe ?HfEN.P--cXl;Wklz.3j.knpEDRTGhuq aBB,,qnwO&RnhB,d\n",
      "ELV-qsEG!TBIjnR KvW!b\n",
      ",\n",
      "Fkb.k,I-JhRS.\n",
      "MxmU,jojC.SAS\n",
      "MUCFiL!OviAFkgxV-IjNJj,MdXatxVhyk,;dQvXnZVO?esMuGk3-mpg;e,Ltxmh3G?e:e?WAp\n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(model, torch.zeros((1,1), dtype=torch.long), 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) ## Much more advanced than SGD; Typical lr is 1e-4, but for smaller NNs we can use much higher value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4031589f8a74438b49948346e1bcb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "num_steps = 100\n",
    "pbar = tqdm.tqdm(desc=\"train\", total=num_steps)\n",
    "for step in range(num_steps):\n",
    "    xb, yb = get_batch(\"train\", batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({\"loss\": loss.detach().item()})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.auto as tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab: {''.join(chars)}\")\n",
    "# print(f\"{len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx = {}\n",
    "idx2token = {}\n",
    "\n",
    "for idx, token in enumerate(sorted(list(set(text)))):\n",
    "    token2idx[token] = idx\n",
    "    idx2token[idx] = token\n",
    "\n",
    "def encode_one(token):\n",
    "    return token2idx[token]\n",
    "\n",
    "def decode_one(idx):\n",
    "    return idx2token[idx]\n",
    "\n",
    "def encode(text):\n",
    "    return [encode_one(char) for char in text]\n",
    "\n",
    "def decode(code):\n",
    "    return \"\".join([decode_one(idx) for idx in code])\n",
    "\n",
    "vocab_size = len(token2idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split=\"train\", batch_size=4, block_size=8, device=None):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    batch_offsets = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "    xs = torch.stack([data[idx:idx+block_size] for idx in batch_offsets])\n",
    "    ys = torch.stack([data[idx+1:idx+block_size+1] for idx in batch_offsets])\n",
    "    if device:\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        logits = self.token_emb(inputs) ## logits.shape=(B,T,C)\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id += 1\n",
    "exp_name = f\"shakespeare-bigram-{run_id}\"\n",
    "writer = SummaryWriter(f\"runs/{exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25de385adb54a02ba967666a7b879ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) ## Much more advanced than SGD; Typical lr is 1e-4, but for smaller NNs we can use much higher value\n",
    "\n",
    "num_steps = 10000\n",
    "pbar = tqdm.tqdm(desc=\"train\", total=num_steps)\n",
    "for step in range(num_steps):\n",
    "    xb, yb = get_batch(\"train\", batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_val = loss.item()\n",
    "    optimizer.step()\n",
    "    pbar.set_postfix({\"loss\": loss_val})\n",
    "    writer.add_scalar(\"Loss/train\", loss_val, step)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BuORI ck lifad:\n",
      "Opicey helersurke ngorasechiveend ofoxFr seis are,'D:\n",
      "I y es ast py tXJus at ADre crwchiuthe t\n",
      "GEWBUMapre t fellrerin ton'semye, yome and th horsthuthoghegis hes tind, o.\n",
      "Wheneel wooe it as d 'd vo's y ge,\n",
      "Thioutong d,\n",
      "S ty ildeanst, ntichand aran'd itas foy t, w pring the s ind:\n",
      "BRDI yorig thal ar VI for tharey PXEre\n",
      "DUK:\n",
      "S:\n",
      "Ha w f y adotrs the lyous y y, athee igVimoro-have t\n",
      "E:\n",
      "IOLed:\n",
      "Plontor, mey e nd coro co shabe y athat! sd waut ju w'd nm mey he ffone yo by curoftha CHAN I\n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(model, torch.zeros((1,1), dtype=torch.long), 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cpu\"\n",
    "    batch_size: int = 32\n",
    "    block_length: int = 8\n",
    "    num_steps: int = 10000\n",
    "    learning_rate: float = 1e-3\n",
    "    eval_freq: int = 1000\n",
    "    eval_batches: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.token_emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        logits = self.token_emb(inputs) ## logits.shape=(B,T,C)\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(config):\n",
    "    if config.device == \"gpu\" and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif config.device == \"gpu\" and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, num_batches):\n",
    "    result = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(num_batches)\n",
    "        for batch_id in range(num_batches):\n",
    "            xs, ys = get_batch(split)\n",
    "            _, loss = model(xs, ys)\n",
    "            losses[batch_id] = loss.item()\n",
    "        result[split] = losses.mean()\n",
    "    model.train()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, run_name, config: TrainConfig):\n",
    "    print(f\"executing train run: {run_name}\")\n",
    "    device = get_device(config)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    pbar = tqdm.tqdm(desc=\"train\", total=config.num_steps)\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    for step in range(config.num_steps):\n",
    "        xb, yb = get_batch(\"train\", batch_size=config.batch_size, block_size=config.block_length, device=device)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_val = loss.detach().item()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({\"loss\": loss_val})\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", loss_val, step)\n",
    "\n",
    "        if step % config.eval_freq == 0:\n",
    "            losses_smooth = estimate_loss(model, num_batches=config.eval_batches)\n",
    "            writer.add_scalar(\"AvgLoss/train\", losses_smooth[\"train\"], step)\n",
    "            writer.add_scalar(\"AvgLoss/val\", losses_smooth[\"val\"], step)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d4906fdb7c4c64a10a20ccb6046423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001)\n",
    "model = BigramLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math trick to quickly calculate self-attention via MatMul + Softmax\n",
    "Allows tokens to start talking to eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.3596,  0.6258,  0.9545,  0.3612, -1.3499,  0.2360, -0.9211])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xmean = torch.mean(xprev, 0) # (C,)\n",
    "        xbow[b,t] = xmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0894,  0.1490,  0.3504,  0.3525,  0.0688,  0.0927, -0.0341])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.triu(torch.ones((T,T)))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0894,  0.1490,  0.3504,  0.3525,  0.0688,  0.0927, -0.0341])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch way to get moving average\n",
    "xbow2 = ((x.permute((0,2,1)) @ ones) / ones.sum(0)).permute(0,2,1)\n",
    "xbow2[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0894,  0.1490,  0.3504,  0.3525,  0.0688,  0.0927, -0.0341])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0894,  0.1490,  0.3504,  0.3525,  0.0688,  0.0927, -0.0341])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(xbow[0,:,0], xbow2[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted aggregation via broadcasted batch multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = torch.tril(torch.ones((T,T)))\n",
    "weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "xbow3 = weights @ x # (T,T) @ (B,T,C) => Broadcasts weights so that it has B dimension: (B,T,T) @ (B,T,C) => (B,T,C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make attention mask via softmax. \n",
    "Currnetly weigiths are initialized with zeros, but next they are going to be learned from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.zeros(T,T)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "xbow4 = weights @ x # (T,T) @ (B,T,C) => Broadcasts weights so that it has B dimension: (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "torch.allclose(xbow, xbow4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugging it back to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cpu\"\n",
    "    batch_size: int = 32\n",
    "    block_length: int = 8\n",
    "    num_steps: int = 10000\n",
    "    learning_rate: float = 1e-3\n",
    "    eval_freq: int = 1000\n",
    "    eval_batches: int = 100\n",
    "    token_emb_size: int = 32   ## Added this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        # This creates one spurious level of interaction through the linear layer:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "        logits = self.lm_head(tok_emb) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-lmhead-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9844dcdd387545edae95f5e1dcfbf29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-lmhead-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001)\n",
    "model = BigramLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-posemb-4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8cf56cb529463eaa4e5245db9b3643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-posemb-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001)\n",
    "model = BigramLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement self-attention for single individual \"head\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single token (aka node) at each position will emit two vectors:\n",
    "- Query: What am I looking for\n",
    "- Key: What do I contain\n",
    "\n",
    "The way we get affinities between tokens in the sequence is: we basically do the dot product between the keys and the queries - so my query dot products with all the keys of the other tokens in the sequence. And that dot product now becomes values in `weights` matrix (before we apply masking & softmax). So if the key and query are aligned, they interact in a very high amount, and then we get to learn more about that specific token as opposed to any other tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C) ## batch size, time, channels\n",
    "\n",
    "head_size = 16\n",
    "# bias=False is going to just apply some MatMul with some fixed weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# when we forward this linear on top of my x, all the tokeins in all the positions in the (B,T) arrangement in parallel produce the key and a query\n",
    "## We emit all the keys & queries in parallel (no communication happened just yet):\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -3.3334, -1.0226,  0.7836, -1.2566, -0.3126,  1.0876, -1.8044],\n",
       "        [-1.3011, -1.6556, -1.2606, -0.8014,  0.0187,  2.4152,  1.9652, -0.4126],\n",
       "        [ 0.5652,  0.1040,  0.0762, -0.3368, -0.7880, -0.1106, -0.2621, -0.8306],\n",
       "        [ 2.1616,  3.3782, -0.3813, -0.8496, -1.3204, -0.9931, -0.3158,  0.5898],\n",
       "        [-1.0674, -2.1825, -0.9843, -0.5602,  2.0363,  3.3449,  0.6091, -0.7987],\n",
       "        [ 1.9632,  1.0415, -1.4303, -1.1701,  0.8638, -2.5229,  1.2616, -0.5856],\n",
       "        [ 1.0765, -0.0557,  0.0749, -1.2927,  0.3719,  1.4187, -0.5484,  0.6433],\n",
       "        [-0.4530,  0.2927, -0.9547, -1.0260,  0.9258,  1.2196,  0.8048,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k @ q.transpose(-2, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k.transpose(-2, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -3.3334, -1.0226,  0.7836, -1.2566, -0.3126,  1.0876, -1.8044],\n",
       "        [-1.3011, -1.6556, -1.2606, -0.8014,  0.0187,  2.4152,  1.9652, -0.4126],\n",
       "        [ 0.5652,  0.1040,  0.0762, -0.3368, -0.7880, -0.1106, -0.2621, -0.8306],\n",
       "        [ 2.1616,  3.3782, -0.3813, -0.8496, -1.3204, -0.9931, -0.3158,  0.5898],\n",
       "        [-1.0674, -2.1825, -0.9843, -0.5602,  2.0363,  3.3449,  0.6091, -0.7987],\n",
       "        [ 1.9632,  1.0415, -1.4303, -1.1701,  0.8638, -2.5229,  1.2616, -0.5856],\n",
       "        [ 1.0765, -0.0557,  0.0749, -1.2927,  0.3719,  1.4187, -0.5484,  0.6433],\n",
       "        [-0.4530,  0.2927, -0.9547, -1.0260,  0.9258,  1.2196,  0.8048,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k @ q.transpose(-2, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All the keys will dot-product with all the keys:\n",
    "# weights = k @ q.permute(0, 2, 1)\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) => (B, T, T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # implicit broadcast of (tril == 0) to to (B, T, T)\n",
    "weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "out = weights @ x # (B,T,T) @ (B,T,C) => Broadcasts weights so that it has B dimension: (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
       "         0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
       "         1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
       "         0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4562, -1.0917, -0.8207,  1.8634,  0.8148, -0.0643,  1.4237,  0.2617,\n",
       "        -1.8528,  0.2019, -1.1787, -0.1036, -1.7830, -0.8323, -0.4346, -1.2480,\n",
       "        -0.2880,  0.8809, -0.7190,  0.1745,  0.7520, -0.0629, -0.7111,  0.9810,\n",
       "        -0.7244, -1.5010, -2.8348, -2.8272, -0.1736,  0.0512, -0.6576, -2.5729],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for every row of B we will have (T, T) matrix giving us the affinities, and we're using they as weights in the weighted aggregation.\n",
    "So token knows what content it has and what position it is in. So it creates a query, and all the other tokens are emitting keys.\n",
    "\n",
    "Token & query key & query embeddings may encode various information in their different channels. So the current token emits this information in the query. When all the nodes emit keys, they also advertise this information about themselves. And maybe one of the channels in the query can be, for example: I am a vowel in pos 8 and I am looking for a any conconsonant at position up to four. And now some token may say in it's key: hey, i'm a consonant and i'm in the position 3. And this key will have a higher number in that specific channel. Hence via dot-product the query and a key can find eachother and create a high affinity, and through the softmax we will end up aggregating a lot of it's information into my position, and so I'll get to learn a lot about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we're not aggregating tokens exactly, we're actually projecting into a value space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C) ## batch size, time, channels\n",
    "\n",
    "head_size = 16\n",
    "# bias=False is going to just apply some MatMul with some fixed weights\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) => (B, T, T)\n",
    "## This matmul will run in parallel across batch dimension, so it is similar to looping over rows and multiplying (T, head_size) x (head_size, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # implicit broadcast of (tril == 0) to to (B, T, T)\n",
    "weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "\n",
    "\n",
    "out = weights @ v # (B,T,T) @ (B,T,C) => Broadcasts weights so that it has B dimension: (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a communication mechanism - nodes in directed graph, every node has vector of information, and it aggregates all information from all nodes that point to it.\n",
    "First node is only pointing to itself; Second points to self and prev node etc, all the way up to last node that points back.\n",
    "That's the structure of our directed graph has in our auto-regressive scenario like language modelling.\n",
    "In principle Attention can be applied to any arbitrary directed graph, and it's just a communication mechanism between the nodes.\n",
    "\n",
    "There's no notion of space, nodes have no idea where are they located in space. That's why we need to encode them positionally and give them some information that anchors them to some position to let them know where they are. \n",
    "\n",
    "It's unlike convolutional filters, that are acting in space.\n",
    "If we need it to have notion of space to Attention mechanism, we have to specifically add it.\n",
    "\n",
    "In this case tokens don't talk to future tokens, but in principle we may allow them to, and have the full communication graph.\n",
    "For example: in sentiment analysis we predict the sentiment of the sentence, so it's fine to let tokens talk to eachother. \n",
    "And in this case we just use encoder block of self-attention here: deleting the masked_fill line of code.\n",
    "And what we've implemented here is actually a decoder block, where we want to use masking to avoid giving up the answer.\n",
    "Attention doesn't care, as it supports arbitrary connectivity between nodes.\n",
    "\n",
    "Q: What's the difference between Attention, Self-Attention and Cross-attention?\n",
    "A: We just implemented Self-Attention. The reason it is called this way is because the keys, queries and values are all coming from the same source x.\n",
    "So these nodes are self-attending. But in principle in encoder-decoder transformers we can have queries are produced from input x, but keys and values can come from whole separate external source, and sometimes from encoder blocks that encode some context that we like to condition on.\n",
    "So cross-attention is used when there's a separate source of nodes we'd like to pull information from into our nodes. And it's self-attention if we just have nodes that would like to look to eachother and talk to eachother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding normalization (scaled attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important, especially at initialization, for `weight` to be fairly defused.  \n",
    "If weight takes on very positive & very negative values, `softmax` would converge to one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weight = q @ k.transpose(-2, -1) / (head_size ** 0.5)\n",
    "weight.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugging self-attention into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cpu\"\n",
    "    batch_size: int = 32\n",
    "    block_length: int = 8\n",
    "    num_steps: int = 10000\n",
    "    learning_rate: float = 1e-3\n",
    "    eval_freq: int = 1000\n",
    "    eval_batches: int = 100\n",
    "    token_emb_size: int = 32\n",
    "    head_size: int = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.keys = nn.Linear(config.token_emb_size, config.head_size, bias=False)\n",
    "        self.queries = nn.Linear(config.token_emb_size, config.head_size, bias=False)\n",
    "        self.values = nn.Linear(config.token_emb_size, config.head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(config.head_size, config.head_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.shape\n",
    "\n",
    "        k = self.keys(inputs) # (B,T,H)\n",
    "        q = self.queries(inputs) # (B,T,H)\n",
    "        \n",
    "        ## Compute attention scores, aka affinities:\n",
    "        weights = q @ k.transpose(-2, -1) # (B,T,H) @ (B,H,T) => (B,T,T)\n",
    "        weights = weights / (self.config.token_emb_size ** 0.5) # (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B,T,T) <= This makes it a decoder block\n",
    "        weights = torch.softmax(weights, dim=-1) # (B,T,T)\n",
    "\n",
    "        ## Perform the weighted aggregation of the values:\n",
    "        v = self.values(inputs) # (B, T, head_size)\n",
    "        out = weights @ v # (B,T,T) @ (B,T,H) => (B,T,H)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramSelfAttentionLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_head = Head(config) ## Self-attention head\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_head(x) ## Apply self-attention head\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-selfattention-5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ca2610e2f84e0db4e04ac1fbd668b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-selfattention-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=32)\n",
    "model = BigramSelfAttentionLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G RIED:\n",
      "thounde\n",
      "'Ythe tarcthirighran dsrgalle, dteschanee he gh alemealsle chi'sble ngoushe havele,\n",
      "Nothigroveerce mure, therall, Moug brand Sod,\n",
      "Why har,\n",
      "Toonot; ngsilley arugenaccth\n",
      "I:\n",
      "Thoue tis scar dt yanegnshe? Sharis thild,'d thorsce, hod;\n",
      "Whee din thee ndor theod fers hal I:\n",
      "Cang dizorot nelerd quuinsork de it at, be boran becarlde grorne avesis ay lde ay ha'di fre to; ord wheatiflen o'dy I bulan sbenouve se dy.\n",
      "\n",
      "OF sour the tath he;\n",
      "Ot! waly mem rors me feer daredsoe ud mcow hin herigr, \n"
     ]
    }
   ],
   "source": [
    "print(decode(\n",
    "    generate(\n",
    "        model, \n",
    "        torch.zeros((1,1), dtype=torch.long), \n",
    "        sequence_length=500,\n",
    "        context_size = config.block_length\n",
    "    )[0].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-selfattention-6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0496031dcc4b53aefc1ac9d2413682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-selfattention-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=1e-3, token_emb_size=32, head_size=32)\n",
    "model = BigramSelfAttentionLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got down to a loss of ~2.35 roughly (see tensorboard), let's try to add MultiHead Attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cpu\"\n",
    "    batch_size: int = 32\n",
    "    block_length: int = 8\n",
    "    num_steps: int = 10000\n",
    "    learning_rate: float = 1e-3\n",
    "    eval_freq: int = 1000\n",
    "    eval_batches: int = 100\n",
    "    token_emb_size: int = 32\n",
    "    head_size: int = 8\n",
    "    num_heads: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return torch.concat([head(inputs) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramMultiHeadSelfAttentionLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_heads = MultiHeadAttention(config) ## Multiple Self-attention heads\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_heads(x) ## Apply multiple self-attention heads => (B,T,head_size*num_heads)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-multihead-selfattention-7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51649a1c435b4fc3b2b101f31e78b656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-multihead-selfattention-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = BigramMultiHeadSelfAttentionLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding multi-head self-attention that did the communication, we went too fast to compute the logits.\n",
    "So the tokens looked at eachother but didn't really had enough time to think about what they found from another tokens.\n",
    "So it makes sense to add some simple feed-forward single layer: simple projection followed by non-linearity.\n",
    "Note that this feed-forward is now applied on a per-token level: so all the tokens now do this independently; self-attention is a communication, but once we gathered all the data, we need to think on all this data individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.token_emb_size, config.token_emb_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramMultiHeadSelfAttentionFFLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_heads = MultiHeadAttention(config) ## Multiple Self-attention heads\n",
    "        self.ffwd = FeedForward(config)\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_heads(x) # Apply self-attention heads => (B,T,head_size*num_heads)\n",
    "\n",
    "        x = self.ffwd(x) # (B,T,token_emb_size)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-multihead-selfattention-ff-8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba0010e2b5d48818ab546d17cda047e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-multihead-selfattention-ff-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = BigramMultiHeadSelfAttentionFFLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersperse communication and computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a block that contains self-attention with feed-forward on top of that, and then repeat it multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(config) ## Communication\n",
    "        self.feed_forward = FeedForward(config) ## Computation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.feed_forward(self.self_attention(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramMultiHeadSelfAttentionBlocksLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_blocks = nn.Sequential(\n",
    "            Block(config),\n",
    "            Block(config),\n",
    "            Block(config),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_blocks(x) # Apply self-attention blocks => (B,T,head_size*num_heads)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-multihead-selfattention-blocks-9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edb4249056541448aa3e687b154e09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-multihead-selfattention-blocks-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = BigramMultiHeadSelfAttentionBlocksLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our network starts becoming deeper. And deep NNs are typically suffer from optimization issues, so we need one more idea from the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Skip Connections aka Residual Connections, and Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Resudual pathway we're forking an additional computation graph, perform some computations, and then project back to the residual pathway via addition.\n",
    "So we go from the inputs to the targets only via plus ... plus ... plus ops. During back-prop addition distributes gradients equally to both of it's branches.\n",
    "And this allows supervision - the gradients from the loss - hop through every addition node all the way to the input, and then also fork off into residual blocks. This is basically a gradient super-highway that goes all the way from loss to input unimpeded. And these residual blocks are typically initialized in a way so that they are contributing very little if anything to the residual pathway. During the optimization they come online over time and start to contribute, as they kick in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFeedForward(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.token_emb_size, config.token_emb_size),\n",
    "            nn.ReLU(),\n",
    "            ## We need to add this to have a projection back into a residual pathway:\n",
    "            nn.Linear(config.token_emb_size, config.token_emb_size),\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "\n",
    "        ## We need to add this to have a projection back into a residual pathway:\n",
    "        self.proj = nn.Linear(config.token_emb_size, config.token_emb_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = torch.concat([head(inputs) for head in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.self_attention = ResidualMultiHeadAttention(config) ## Communication\n",
    "        self.feed_forward = ResidualFeedForward(config) ## Computation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        ## Adding a projection back into a residual pathways:\n",
    "        x = x + self.self_attention(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBigramMultiHeadSelfAttentionLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_blocks = nn.Sequential(\n",
    "            ResidualBlock(config),\n",
    "            ResidualBlock(config),\n",
    "            ResidualBlock(config),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_blocks(x) # Apply self-attention blocks => (B,T,head_size*num_heads)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-residual-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44f7760e6034ef7a8a96c26f1f2d1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-residual-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = ResidualBigramMultiHeadSelfAttentionLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply inner dim of FFN by four:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFeedForwardWide(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.token_emb_size, config.token_emb_size * 4),\n",
    "            nn.ReLU(),\n",
    "            ## We need to add this to have a projection back into a residual pathway:\n",
    "            nn.Linear(config.token_emb_size * 4, config.token_emb_size),\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockWide(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.self_attention = ResidualMultiHeadAttention(config) ## Communication\n",
    "        self.feed_forward = ResidualFeedForwardWide(config) ## Computation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        ## Adding a projection back into a residual pathways:\n",
    "        x = x + self.self_attention(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBigramMultiHeadSelfAttentionLanguageModelWide(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_blocks = nn.Sequential(\n",
    "            ResidualBlockWide(config),\n",
    "            ResidualBlockWide(config),\n",
    "            ResidualBlockWide(config),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_blocks(x) # Apply self-attention blocks => (B,T,head_size*num_heads)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-residual-wide-11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b242083767104e949e96f4d4df4db285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-residual-wide-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = ResidualBigramMultiHeadSelfAttentionLanguageModelWide(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FELeft loid.\n",
      "\n",
      "CORIOLANUS:\n",
      "What not be prasty?\n",
      "\n",
      "Feep thou shalling-staling thinc.'\n",
      "\n",
      "QUEEN ELIZABETHEN EDWARD:\n",
      "Now her fie with is grone!\n",
      "What unse shall to tue\n",
      "trege, thee what speaster anher, brunsen is sto vander, 't prow, let I blawn to have than guay reenbre it and at blife macing have heaves my sucquee:\n",
      "I stabled fabehen on hirse you ceady nock undo bown oath,\n",
      "My say it? a laight toll flitield, of his heirle in him the repron, nowt a 'brow hir on ttewliftle.\n",
      "I treath?\n",
      "\n",
      "CLARYork, be kno, tit \n"
     ]
    }
   ],
   "source": [
    "print(decode(\n",
    "    generate(\n",
    "        model, \n",
    "        torch.zeros((1,1), dtype=torch.long), \n",
    "        sequence_length=500,\n",
    "        context_size = config.block_length\n",
    "    )[0].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to get training loss down to ~1.9, but validation loss is ~2.1, it means we're probably start overfitting a little bit, as the network gets big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization makes sure that across batch dimension any individual neuron has unit gaussian distribution (mean=0, stddev=1).\n",
    "So this normalizes every single column of the batch (by keeping track of the distribution statistics).\n",
    "For the Layer Normalization we're going to normalize rows, not columns. This doesn't require to have any running buffers / moving averages to be calculated and stored to be re-used at inference time. Now our computation can just get applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "module = LayerNorm(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding LayerNorm to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also now it is more common to apply layernorm before the transformations, so the're is a re-shuffling of the layernorms.  \n",
    "It's called a pre-norm formulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormResidualBlockWide(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.self_attention = ResidualMultiHeadAttention(config) ## Communication\n",
    "        self.feed_forward = ResidualFeedForwardWide(config) ## Computation\n",
    "        # Mean and Variance is taken across last token_emb_size dimensions, So our B,T dimensions acts as a batch dimension.\n",
    "        # So this is a per-token transformation that normalizes features and makes it unit-gaussian at initialization.\n",
    "        # But because there are beta and gamma trainable parameters, tokens can become non-gaussian\n",
    "        self.ln1 = nn.LayerNorm(config.token_emb_size)\n",
    "        self.ln2 = nn.LayerNorm(config.token_emb_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        ## Adding a projection back into a residual pathways:\n",
    "        x = x + self.self_attention(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNResidualBigramMultiHeadSelfAttentionLanguageModelWide(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_blocks = nn.Sequential(\n",
    "            LayerNormResidualBlockWide(config),\n",
    "            LayerNormResidualBlockWide(config),\n",
    "            LayerNormResidualBlockWide(config),\n",
    "            nn.LayerNorm(config.token_emb_size)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_blocks(x) # Apply self-attention blocks => (B,T,head_size*num_heads)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-residual-ln-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f465bb5ffc4de6bbc0b4ba83d83d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-residual-ln-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4)\n",
    "model = LNResidualBigramMultiHeadSelfAttentionLanguageModelWide(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we managed to get same results, but not much has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout + Cosmetic changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is needed because as we're gonna scale up the number of parameters in our NN, we're starting to get concerned with overfitting  \n",
    "So we're adding it to get some effect of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cpu\"\n",
    "    batch_size: int = 32\n",
    "    block_length: int = 8\n",
    "    num_steps: int = 10000\n",
    "    learning_rate: float = 1e-3\n",
    "    eval_freq: int = 1000\n",
    "    eval_batches: int = 100\n",
    "    token_emb_size: int = 32\n",
    "    head_size: int = 8\n",
    "    num_heads: int = 4\n",
    "    num_blocks: int = 3\n",
    "    dropout: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutHead(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.keys = nn.Linear(config.token_emb_size, config.head_size, bias=False, device=self.device)\n",
    "        self.queries = nn.Linear(config.token_emb_size, config.head_size, bias=False, device=self.device)\n",
    "        self.values = nn.Linear(config.token_emb_size, config.head_size, bias=False, device=self.device)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(config.block_length, config.block_length, device=self.device)))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.shape\n",
    "\n",
    "        k = self.keys(inputs) # (B,T,H)\n",
    "        q = self.queries(inputs) # (B,T,H)\n",
    "        \n",
    "        ## Compute attention scores, aka affinities:\n",
    "        weights = q @ k.transpose(-2, -1) # (B,T,H) @ (B,H,T) => (B,T,T)\n",
    "        weights = weights / (self.config.token_emb_size ** 0.5) # (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B,T,T) <= This makes it a decoder block\n",
    "        weights = torch.softmax(weights, dim=-1) # (B,T,T)\n",
    "\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        ## Perform the weighted aggregation of the values:\n",
    "        v = self.values(inputs) # (B, T, head_size)\n",
    "        out = weights @ v # (B,T,T) @ (B,T,H) => (B,T,H)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.device = get_device(config)\n",
    "        self.heads = nn.ModuleList([DropoutHead(config) for _ in range(config.num_heads)])\n",
    "\n",
    "        ## We need to add this to have a projection back into a residual pathway:\n",
    "        self.proj = nn.Linear(config.token_emb_size, config.token_emb_size, device=self.device)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = torch.concat([head(inputs) for head in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutFeedForward(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.device = get_device(config)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.token_emb_size, config.token_emb_size * 4, device=self.device),\n",
    "            nn.ReLU(),\n",
    "            ## We need to add this to have a projection back into a residual pathway:\n",
    "            nn.Linear(config.token_emb_size * 4, config.token_emb_size, device=self.device),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.device = get_device(config)\n",
    "        self.self_attention = DropoutMultiHeadAttention(config) ## Communication\n",
    "        self.feed_forward = DropoutFeedForward(config) ## Computation\n",
    "        # Mean and Variance is taken across last token_emb_size dimensions, So our B,T dimensions acts as a batch dimension.\n",
    "        # So this is a per-token transformation that normalizes features and makes it unit-gaussian at initialization.\n",
    "        # But because there are beta and gamma trainable parameters, tokens can become non-gaussian\n",
    "        self.ln1 = nn.LayerNorm(config.token_emb_size, device=self.device)\n",
    "        self.ln2 = nn.LayerNorm(config.token_emb_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        ## Adding a projection back into a residual pathways:\n",
    "        x = x + self.self_attention(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutBigramLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = get_device(config)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, config.token_emb_size, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_length, config.token_emb_size, device=self.device)\n",
    "        self.sa_blocks = nn.Sequential(*[\n",
    "            DropoutBlock(config) for _ in range(config.num_blocks)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.token_emb_size, device=self.device)\n",
    "        self.lm_head = nn.Linear(config.token_emb_size, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputs, targets=None): ## inputs.shape=(B,T), targets.shape=(B,T)\n",
    "        \n",
    "        B,T = inputs.shape\n",
    "        \n",
    "        ## Encoding the tokens:\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,token_emb_size)\n",
    "\n",
    "        ## Sometimes people also encode positions of the tokens\n",
    "        pos_idx = torch.arange(T, device=self.device) # T\n",
    "        pos_emb = self.position_embedding_table(pos_idx) # (T,token_emb_size)\n",
    "\n",
    "        x = tok_emb + pos_emb # pos_emb gets broadcasted across batch => (B,T,token_emb_size)\n",
    "\n",
    "        x = self.sa_blocks(x) # Apply self-attention blocks => (B,T,head_size*num_heads)\n",
    "        x = self.ln_f(x) # Apply final layer normalization\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            (B,T,C) = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-dropout-05-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362499b199bb4273a68ee07ea545e427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-dropout-05-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4, dropout=0.5)\n",
    "model = DropoutBigramLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-dropout-02-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2a79a550a343d1adb69294b64288fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-dropout-02-{run_id}\"\n",
    "config = TrainConfig(device=\"cpu\", num_steps=10_000, learning_rate=0.001, token_emb_size=32, head_size=8, num_heads=4, dropout=0.2)\n",
    "model = DropoutBigramLanguageModel(config)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up a network size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing train run: shakespeare-bigram-big-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b392a5bd8c463fbc26ff4be5d6a7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[39m=\u001b[39m DropoutBigramLanguageModel(config)\n\u001b[1;32m     18\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m run_train(model, run_name, config)\n",
      "Cell \u001b[0;32mIn[64], line 20\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(model, run_name, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLoss/train\u001b[39m\u001b[39m\"\u001b[39m, loss_val, step)\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39meval_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     losses_smooth \u001b[39m=\u001b[39m estimate_loss(model, num_batches\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49meval_batches)\n\u001b[1;32m     21\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mAvgLoss/train\u001b[39m\u001b[39m\"\u001b[39m, losses_smooth[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], step)\n\u001b[1;32m     22\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mAvgLoss/val\u001b[39m\u001b[39m\"\u001b[39m, losses_smooth[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m], step)\n",
      "File \u001b[0;32m~/Documents/code/nanoGPT/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[63], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, num_batches)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m batch_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[1;32m      8\u001b[0m     xs, ys \u001b[39m=\u001b[39m get_batch(split)\n\u001b[0;32m----> 9\u001b[0m     _, loss \u001b[39m=\u001b[39m model(xs, ys)\n\u001b[1;32m     10\u001b[0m     losses[batch_id] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m result[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/Documents/code/nanoGPT/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[136], line 19\u001b[0m, in \u001b[0;36mDropoutBigramLanguageModel.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     16\u001b[0m B,T \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape\n\u001b[1;32m     18\u001b[0m \u001b[39m## Encoding the tokens:\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tok_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding_table(inputs) \u001b[39m# (B,T,token_emb_size)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m## Sometimes people also encode positions of the tokens\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pos_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(T, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m# T\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/nanoGPT/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/code/nanoGPT/.venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/Documents/code/nanoGPT/.venv/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "run_id += 1\n",
    "run_name = f\"shakespeare-bigram-big-{run_id}\"\n",
    "config = TrainConfig(\n",
    "    device=\"gpu\", \n",
    "    batch_size=64, ## Number of independent sequences to be processed in parallel\n",
    "    block_length=256, ## Maximum context length for predictions\n",
    "    num_steps=10_000, \n",
    "    eval_batches=200,\n",
    "    eval_freq=500,\n",
    "    learning_rate=3e-4,\n",
    "    token_emb_size=384, \n",
    "    head_size=384 // 6, \n",
    "    num_heads=6,\n",
    "    dropout=0.2\n",
    ")\n",
    "device = get_device(config)\n",
    "model = DropoutBigramLanguageModel(config)\n",
    "model.to(device)\n",
    "run_train(model, run_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails on mac now, but on A100 could take ~15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper there's also an encoder part because it's a language translation paper. An encoder transformer gets an input (prompt) in a different language, embeds it's tokens via similar transformer, but with unmasked self-attention, and then via cross-attention allows decoder to attend to it.\n",
    "\n",
    "To train smth similar to Chat-GPT there's a pre-training and fine-tuning steps. In a pre-training stage we are training on a large chunk of internet and just trying to get a decoder-only transformer to babble text. Similar to what we've done here. Differences: Our little Shakespeare transformer is about 10m parameters, and the dataset is about 1m characters, so roughly 1m tokens. OpenAI uses subword tokenizer, so their vocabulary is of ~50k elements. And their sequences are more condensed. In this vocabulary Shakespeare dataset would be ~ 300k tokens. \n",
    "\n",
    "So we trained 10m parameter model on 300k tokens. Biggest GPT3 transformer has:\n",
    "- 175B parameters\n",
    "- 96 layers\n",
    "- token_emb_size: 128 * 96 = 12288\n",
    "- head_size = 128\n",
    "- num_heads = 96\n",
    "- batch_size = 3.2m\n",
    "- learning_rate = 6e-5\n",
    "\n",
    "They trained on 300 billion tokens. And that is not considered big anymore.\n",
    "Architecture is actually pretty similar to what we looked at it.\n",
    "\n",
    "After pre-training stage we get a document completer. It tries to complete sequences, so from questions it would ask more questions.\n",
    "\n",
    "Second fine-tuning stage is - align it. 3 steps:\n",
    "\n",
    "Step 1. Fine-Tuning\n",
    "- Collect training data that looks like a use-case (documents with questions on top, and answers in below)\n",
    "- They then fine-tune the model to only focus on documents that look like these\n",
    "- These very-very large models are very sample-efficient during fine-tuning\n",
    "\n",
    "Step 2. Let model respond. Responses are getting ranked to collect a training set that is then used to train a different reward model that predicts how much a candidate response will be desirable.\n",
    "\n",
    "Step 3. Once we have a reward model, we run PPO - form of a policy gradient reinforcement learning optimizer to fine-tune this sampling policy so that answers generated are expected to score high according to a reward model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
